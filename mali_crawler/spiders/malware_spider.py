# malware_spider.py
import scrapy
from urllib.parse import urljoin

class MalwareSpider(scrapy.Spider):
    name = "malware_crawler"
    allowed_domains = ["localhost:5001"]
    custom_settings = {
        'FILES_STORE': '/path/to/safe/storage',
        'ITEM_PIPELINES': {'scrapy.pipelines.files.FilesPipeline': 1}
    }

    def start_requests(self):
        yield scrapy.Request("http://localhost:5001/malware", self.parse)

    def parse(self, response):
        download_link = response.css('a.btn-download::attr(href)').get()
        if download_link:
            yield {
                'file_urls': [urljoin(response.url, download_link)],
                'page_url': response.url
            }
