% This section should include a broader overview of the work. All in all, spend a
% bit more time covering the elements of the abstract, and outline the work in a bit more depth.

% An introduction should help me understand what you did, why you did it the way
% you did, and why do you think the work is \emph{sound}.

% This section should probably be half a page to $\frac{3}{4}$ of a page.
%% introduction.tex
% \section{Introduction}
Web crawlers play a critical role in modern internet applications, enabling automated data collection for search engines, market research, academic studies, and numerous business processes. Scrapy, a popular open-source crawling framework, is widely adopted due to its speed, flexibility, and scalability. However, while much focus has been placed on its performance and extensibility, relatively little attention has been paid to its security robustness when encountering malicious web content.

Despite their importance, web crawlers like Scrapy remain vulnerable to classic web-based threats such as SQL Injection (SQLi), Cross-Site Scripting (XSS), and Malware Downloads. A compromised crawler could become an unintentional attack vector, exposing internal systems to malware, data leaks, or system compromise. Previous security breaches highlight the potential severity of these vulnerabilities when untrusted input is processed without adequate sanitization or validation.

Prior research has addressed web crawler performance optimizations and domain-specific crawling strategies, but comprehensive security assessments remain rare. Limited efforts such as black-box vulnerability scanners have explored detecting XSS or SQLi vulnerabilities from the application side, but few have examined how crawlers themselves handle hostile environments. This gap leaves practitioners without clear guidance on how resilient common frameworks like Scrapy are against active adversaries.

In this project, we designed and implemented a sandbox environment consisting of a malicious Flask web server hosting vulnerable endpoints for SQLi, XSS, and malware download attacks. We developed customized Scrapy spiders to interact with these attack vectors, capturing how Scrapy processes potentially dangerous responses. The system includes live logging, auto-refreshing dashboards, and asynchronous triggering of crawls via a simple web interface.

Our experiments revealed critical vulnerabilities in Scrapy’s handling of SQL Injection (SQLi) payloads. In multiple test cases, Scrapy submitted crafted login forms containing SQLi attack strings, and the framework processed the server's responses without any warnings, validations, or exceptions. This suggests that Scrapy is vulnerable to leaking sensitive backend data or unintentionally assisting in SQL exploitation when interacting with vulnerable sites.

In addition to SQLi and malware download tests, we evaluated Scrapy’s resilience against reflected XSS vectors by integrating a headless browser into our crawling pipeline. Our spider replayed a curated list of malicious payloads—ranging from simple script injections to SVG event-handlers, data-URI scripts, and autofocus triggers—against a Flask endpoint that naively echoes the name parameter into the DOM. We injected a tiny “shim” via Playwright’s add_init_method API to override window.alert(), capturing any execution rather than blocking on a real dialog. 

This few XSS success underscores a broader risk: when Scrapy is augmented with JavaScript rendering (for DOM-based discovery or screenshotting), any reflected script in a target page can run under the crawler’s browser context. By default, Scrapy does not sanitize or warn about inline scripts in the responses it processes, nor does it inspect event-handlers that auto-fire on load. A malicious site could exploit this to exfiltrate data or pivot into internal networks via the crawler’s runtime environment. We therefore recommend that practitioners (1) strictly escape or sanitize reflected user input on the server side, (2) employ a robust Content Security Policy to disallow inline event handlers and data URIs, and (3) if rendering is required, isolate each page in a locked-down browser context with no sensitive host access.

Our experiments also examined how Scrapy handles exposure to web-based malware, specifically the risk of malware file downloads during automated crawling. In our sandbox, the Flask server presented download links for executable files (e.g., MalwareSimulation.exe and various .zip archives) designed to mimic real-world malware distribution tactics. The Scrapy spider was tasked with discovering and downloading these files as part of its normal crawling process.

We found that Scrapy, by default, does not distinguish between benign and potentially malicious file downloads. When encountering links to executable files or archives, the framework fetches and stores these files without any built-in validation, warning, or sandboxing. This behavior is not unique to Scrapy-most web crawlers treat all downloadable content as data-but it introduces significant security risks. If a crawler is operated on a workstation or server with insufficient isolation, inadvertently downloaded malware could be executed by a user or automated process, leading to possible system compromise.
