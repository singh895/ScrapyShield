% A discussion section is usually a place where you explore further implications of your work.
% Take, for example, the findings could have implications about other work.

% \subsection{Implementation challenges}
% Tell me if someting you did didn't go your way.
% Here you can convince me that there were unforseen circumstances, and whether you adjusted in the best way possible.
% For example, you were trying to implement this using library X but library X isn't compatible with the codebase you were working with.

% \subsection{Alternative Approaches}
% This is also good place to talk about why what you did didn't work, as well as other approaches you tried and why they didn't work.

% \subsection{Community Interactions}
% If you spoke with OSS contributors, I recommend spending 1-2 paragraphs here outlining the interaction. Did you learn something? was it easy? hard? do you think they will take your contribution if we gave it more months? did they already take it?

% discussion.tex
% \section{Discussion}
% Our experiments expose Scrapy’s passive stance toward hostile content. In SQLi tests, raw payloads passed unchanged to the server; in XSS tests, over 40\% of event‐handler and media‐error vectors executed in the headless browser; and in malware tests, every executable and archive was downloaded without validation.  

% \subsection{Implementation Challenges}
% Integrating Playwright into Scrapy required upgrading to an asynchronous middleware and managing browser contexts. We also encountered intermittent timeouts when large binaries were fetched, forcing us to tune Scrapy’s download and concurrency settings.

% \subsection{Alternative Approaches}
% We trialed a pure‐Requests approach for XSS detection (inspecting raw HTML for `<script>` tags) but found too many false positives/negatives. A hybrid approach—combining static HTML analysis with selective JS rendering—may strike a better balance.

% \subsection{Community Interactions}
% We opened an issue and PR on the `scrapy-playwright` repository to request built‐in CSP bypass controls; maintainers were responsive but noted the need for more test cases. We also consulted the OWASP community on best practices for crawler‐side CSP enforcement.
The SQL Injection results highlight significant security risks associated with using Scrapy for crawling untrusted or semi-trusted domains. In practice, attackers could exploit vulnerable endpoints discovered during broad crawls, and Scrapy would not provide any alerts or blocks against interacting with malicious inputs or results. While Scrapy offers excellent flexibility for data extraction, its default configuration does not prioritize adversarial resilience.
Compared to existing tools such as WVF [4] or OWASP ZAP [6], which actively scan for vulnerabilities, Scrapy operates passively and assumes benign environments. Our findings suggest that integrating security middleware—such as input sanitization, response inspection, or payload pattern matching—would significantly improve Scrapy’s suitability for real-world deployments in hostile web ecosystems.

The Malware download experiments further demonstrate Scrapy's lack of security-oriented design when handling potentially malicious file content. Unlike dedicated security tools such as ClamAV or network sandboxing solutions that inspect file contents or limit execution risks, Scrapy treats all downloadable content equally-extracting and storing potentially harmful executables without any validation, scanning, or isolation mechanisms. This behavior creates a significant risk vector where an automated crawler could inadvertently become a malware distribution channel within an organization's infrastructure.
Our findings align with CVE-2024-3572, which identified a decompression bomb vulnerability in Scrapy that allows malicious sites to exhaust system resources. This vulnerability underscores a broader pattern where Scrapy prioritizes extraction efficiency over security resilience. For enterprise deployments, this necessitates implementing additional defensive layers such as pre-download reputation checking, post-download malware scanning, and isolated execution environments. Organizations employing Scrapy at scale should recognize that the framework implicitly transfers security responsibility to the implementation team rather than providing native protections against hostile content.

Our Cross-Site Scripting experiments reveal that, when Scrapy is extended with a JavaScript rendering engine, reflected payloads can execute freely in the crawler’s browser context. Scrapy itself makes no attempt to sanitize or flag inline <script> tags, event-handler attributes, or resource-error vectors—it simply fetches the raw HTML and passes it to the renderer. In our tests, more than 40 percent of payloads (notably SVG onload, image/video onerror, <details open> toggles, and embedded data-URI SVGs) fired without any intervention. This means a malicious site could embed arbitrary client-side code that runs under Scrapy’s privileges—potentially exfiltrating data, dropping further attack scripts, or pivoting into internal networks if the crawler environment isn’t fully locked down.
By contrast, security-focused tools like OWASP ZAP or Burp Suite actively detect and neutralize XSS by scanning responses, sanitizing inputs, and enforcing policies before rendering. Scrapy’s default pipeline—designed for speed and flexibility—assumes a benign target and provides no built-in defenses against hostile scripts. To safely crawl untrusted or semi-trusted domains, practitioners should introduce a security middleware layer that strips or encodes dangerous tags, enforce a strict Content Security Policy to ban inline handlers and data URIs, and isolate each rendering context in its own sandbox (e.g. a disposable browser profile with no network or file-system privileges). Without these protections, automated scraping can inadvertently become an attack vector, executing arbitrary code on behalf of the crawler.
